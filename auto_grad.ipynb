{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# This will tell python that when there comes the optimization step this tensor will update\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a simple tensor : tensor([0.9721, 0.1230, 2.0197])\n",
      "Printintg Y and We see grad_fn as x has requires_grad_true : \n",
      " The grad_fn  is AddBackward beacause of addittion: tensor([1.8787, 2.4860, 1.0933], grad_fn=<AddBackward0>)\n",
      "Printing Z and the grad_fn is MulBackward : 10.904908180236816\n",
      "The gradient stored in the x variable is : tensor([3.7574, 4.9719, 2.1866])\n"
     ]
    }
   ],
   "source": [
    "# Python Provide an auto_grad package which will do all the computation for us.\n",
    "x = torch.randn(3)\n",
    "print(f\"This is a simple tensor : {x}\")\n",
    "\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "y = x +2\n",
    "\n",
    "print(f'Printintg Y and We see grad_fn as x has requires_grad_true : \\n The grad_fn  is AddBackward beacause of addittion: {y}')\n",
    "\n",
    "z = y*y*3\n",
    "z = z.mean()\n",
    "print(f'Printing Z and the grad_fn is MulBackward : {z}')\n",
    "# When calling this the gradient will stores in .grad attribute of the tensor\n",
    "z.backward() # This is dz/dx\n",
    "\n",
    "print(f\"The gradient stored in the x variable is : {x.grad}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a simple tensor : tensor([ 0.3333, -0.3865,  0.0128])\n",
      "Printintg Y and We see grad_fn as x has requires_grad_true : \n",
      " The grad_fn  is AddBackward beacause of addittion: tensor([3.0231, 3.3032, 1.9195], grad_fn=<AddBackward0>)\n",
      "Printing Z and the grad_fn is MulBackward and it is a tensor of 3 size : tensor([27.4177, 32.7327, 11.0530], grad_fn=<MulBackward0>)\n",
      " Here we also pass the vector which is multiplied by the jacobian matrix \n",
      "The gradient stored in the x variable is : tensor([1.8139, 1.9819, 1.1517])\n"
     ]
    }
   ],
   "source": [
    "# Python Provide an auto_grad package which will do all the computation for us.\n",
    "# In this we see if we have a vector so we remove z.mean()\n",
    "x = torch.randn(3)\n",
    "print(f\"This is a simple tensor : {x}\")\n",
    "\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "y = x +2\n",
    "\n",
    "print(f'Printintg Y and We see grad_fn as x has requires_grad_true : \\n The grad_fn  is AddBackward beacause of addittion: {y}')\n",
    "\n",
    "z = y*y*3\n",
    "print(f'Printing Z and the grad_fn is MulBackward and it is a tensor of 3 size : {z}')\n",
    "\n",
    "v = torch.tensor([0.1,0.1,0.1])\n",
    "# This will basically do the vector jacobian product to find the gradient.\n",
    "z.backward(v) # This is dz/dx\n",
    "\n",
    "print(f\" Here we also pass the vector which is multiplied by the jacobian matrix \\nThe gradient stored in the x variable is : {x.grad}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tensor with required grad true : tensor([-1.0710,  1.1293,  2.1238], requires_grad=True)\n",
      "Option 1 tensor : tensor([-1.0710,  1.1293,  2.1238])\n",
      "Option 2 : tensor([-0.6665, -1.0107,  0.1560])\n",
      "This is y with no grad_fn : tensor([1.0420, 2.5122, 1.7903])\n"
     ]
    }
   ],
   "source": [
    "# There are 3 options by which we can prevent the pytorch to track history\n",
    "\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "print(f\"Simple tensor with required grad true : {x}\")\n",
    "#x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad\n",
    "\n",
    "x.requires_grad_(False)\n",
    "print(f\"Option 1 tensor : {x}\")\n",
    "\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "x = x.detach()\n",
    "print(f\"Option 2 : {x}\")\n",
    "\n",
    "\n",
    "x = torch.randn(3,requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x +2\n",
    "    print(f\"This is y with no grad_fn : {y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Gradients of Weights : tensor([3., 3., 3., 3.])\n",
      "Calculated Gradients of Weights : tensor([3., 3., 3., 3.])\n",
      "Calculated Gradients of Weights : tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Here we are building a simple training loop\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output =  (weights*3).sum()\n",
    "\n",
    "    model_output.backward()\n",
    "    print(f\"Calculated Gradients of Weights : {weights.grad}\")\n",
    "\n",
    "    weights.grad.zero_()\n",
    "\n",
    "# Here we have to note one thing that we have to empty the grad attribute so that whenever new iteration comes it doesnt sum up the previous gradient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
